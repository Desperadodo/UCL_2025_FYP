import os
import pandas as pd
import h5py
import torch
import numpy as np
import random
import ast
import string # Import the string module
from typing import TYPE_CHECKING, Dict, List, Optional, Union
from datasets import Dataset
from transformers import PreTrainedTokenizer
from PIL import Image

from ..extras import logging
from .converter import DatasetConverter
from .data_utils import Role

if TYPE_CHECKING:
    from ..hparams import DataArguments
    from .parser import DatasetAttr

logger = logging.get_logger(__name__)

# ğŸ”§ Stage2ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„é—®é¢˜æ¨¡æ¿ï¼Œä¸“æ³¨äºç®€å•çš„image-captionå¯¹åº”
# å‚è€ƒLLaVAå®˜æ–¹Stage2ä½¿ç”¨CC-3Mçš„ç®€æ´æ ¼å¼

# ğŸ¯ Stage3 VQAé—®å¥æ¨¡æ¿ - ç”¨äºå°†captionè½¬æ¢ä¸ºé—®ç­”å¯¹
VQA_QUESTION_TEMPLATES = [
    "Please provide a comprehensive pathology report for this whole-slide image.",
    "Could you summarise the key microscopic findings shown here?",
    "Give a detailed diagnostic interpretation of the WSI.",
    "What is the formal pathologic diagnosis for this slide?",
    "Describe all notable histologic features present.",
    "Compose the full surgical pathology report based on this specimen.",
    "List the main tumour characteristics observed in the image.",
    "Provide a narrative pathologist's report for this case.",
    "Write the pathology impression section for this WSI.",
    "What conclusions should a pathologist draw from this slide?",
    "Draft the diagnostic comment you would add to the medical record.",
    "Summarise the lesion type, grade and any special findings visible.",
    "Give a concise but complete pathology synopsis.",
    "Create the final diagnostic statement for this whole-slide image.",
    "Please render the pathology diagnosis in standard terminology.",
    "Provide the histopathologic report as if for tumour board review.",
    "Offer a detailed description of tumour morphology and behaviour.",
    "Enumerate all relevant prognostic features seen in the WSI.",
    "Present the full pathologic assessment including margins and stage.",
    "Write the 'Microscopic Findings' section for this specimen.",
    "Summarise any invasive components and associated in-situ changes.",
    "State the tumour subtype and any ancillary findings.",
    "Provide a thorough histological evaluation suitable for the lab report.",
    "Compose an expert pathology note describing everything of importance.",
    "What should the attending pathologist report about this tissue?",
    "Deliver a structured pathology diagnosis for this case.",
    "Give an expert interpretation of the observed pathology.",
    "Summarise this WSI in the style of a formal pathology report.",
    "Please generate the diagnostic comment for this breast specimen.",
    "Describe the tumour and any associated features as seen under the microscope."
]

def make_vqa_question(slide_id: str, random_seed: Optional[int] = None) -> str:
    """
    ä¸ºç»™å®šçš„slide_idç”Ÿæˆä¸€ä¸ªVQAé—®å¥ã€‚
    ä½¿ç”¨slide_idä½œä¸ºéšæœºç§å­ç¡®ä¿åŒä¸€ä¸ªslideæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„é—®å¥ã€‚
    """
    if random_seed is None:
        # ä½¿ç”¨slide_idçš„hashä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿å¯é‡ç°æ€§
        random_seed = hash(slide_id) % (2**32)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(random_seed)
    
    # éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿
    selected_template = random.choice(VQA_QUESTION_TEMPLATES)
    
    # é‡ç½®éšæœºç§å­é¿å…å½±å“å…¶ä»–éšæœºæ“ä½œ
    random.seed(None)
    
    return selected_template

def get_unpuzzle_dataset(
    dataset_name: str,
    dataset_dir: str,
    tokenizer: "PreTrainedTokenizer",
    **kwargs
) -> Dataset:
    """Load and process the Unpuzzle WSI dataset."""
    try:
        # Get dataset attributes
        dataset_attr = kwargs.get("dataset_attr")
        data_args = kwargs.get("data_args")
        
        # ğŸ”§ æ ‡å‡†æ¨¡å¼ï¼šæ ¹æ®dataset_nameç¡®å®šæ•°æ®æ–‡ä»¶
        if dataset_name in ["pathtext_stage2_train", "pathtext_stage2_val"]:
            # æ–°çš„æ ‡å‡†æ¨¡å¼ï¼šç›´æ¥ä»æ•°æ®é›†åç§°ç¡®å®šæ–‡ä»¶
            if dataset_name == "pathtext_stage2_train":
                csv_filename = "PathText-BRCA-unpuzzle-train.csv"
                split_name = "è®­ç»ƒé›†"
            else:  # pathtext_stage2_val
                csv_filename = "PathText-BRCA-unpuzzle-val.csv"
                split_name = "éªŒè¯é›†"
            
            csv_path = f"/data/private_hdd/PathText-unpuzzle/{csv_filename}"
            logger.info(f"ğŸ“Š ä½¿ç”¨ {split_name}: {csv_path}")
        elif dataset_name == "pathtext_stage2":
            # å…¼å®¹æ—§çš„æ–¹å¼ï¼ˆå¦‚æœè¿˜æœ‰åœ°æ–¹ä½¿ç”¨ï¼‰
            split = dataset_attr.split if dataset_attr else 'train'
            csv_path = f"/data/private_hdd/PathText-unpuzzle/PathText-BRCA-unpuzzle-{split}.csv"
            logger.info(f"ğŸ“Š å…¼å®¹æ¨¡å¼ - ä½¿ç”¨ {split} æ•°æ®é›†: {csv_path}")
        else:
            # Construct CSV path for other datasets
            csv_path = os.path.join(dataset_dir, f"{dataset_name}.csv")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        # Read CSV file - handle both comma and tab separators
        try:
            df = pd.read_csv(csv_path, sep=",", keep_default_na=False)  # Try comma first
        except:
            df = pd.read_csv(csv_path, sep="\t", keep_default_na=False)  # Fallback to tab
        
        # å…³é”®ä¿®æ­£ï¼šå°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä»¥é˜²æ­¢pyarrowåœ¨å¤„ç†æ··åˆç±»å‹ï¼ˆstrå’Œfloatï¼‰æ—¶å‡ºé”™ã€‚
        df.fillna("", inplace=True)

        logger.info(f"Loaded {len(df)} samples from {csv_path}")
        logger.info(f"CSV columns: {df.columns.tolist()}")
        
        # ğŸ” å¦‚æœä½¿ç”¨é¢„å…ˆåˆ’åˆ†çš„æ•°æ®ï¼Œè®°å½•åˆ’åˆ†ä¿¡æ¯
        if 'split' in df.columns:
            split_counts = df['split'].value_counts()
            logger.info(f"ğŸ“Š æ•°æ®åˆ’åˆ†ç»Ÿè®¡: {split_counts.to_dict()}")
            # åªä½¿ç”¨è®­ç»ƒæ•°æ®
            if 'train' in split_counts:
                df = df[df['split'] == 'train'].copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨è®­ç»ƒé›†æ•°æ®: {len(df)} ä¸ªæ ·æœ¬")
        
        # Convert DataFrame to list of dicts
        data = []
        csv_dir = os.path.dirname(os.path.abspath(csv_path))
        
        for idx, row in df.iterrows():
            # Handle different CSV formats
            if "filepath" in df.columns and "title" in df.columns:
                # PathText-unpuzzle format: filepath, subset, title
                h5_path = os.path.join("/data/private_hdd/PathText-unpuzzle", row["filepath"])
                caption = row["title"]
                question = ""
                answer = ""
                slide_id = os.path.basename(row["filepath"]).replace(".h5", "")
                dataset_name_field = row.get("subset", "PathText")
            else:
                # Standard format: h5_path, question, answer, caption, etc.
                h5_path = row.get("h5_path", "")
                if not os.path.isabs(h5_path):
                    h5_path = os.path.join(csv_dir, h5_path)
                question = row.get("question", "").strip()
                answer = row.get("answer", "").strip()
                caption = row.get("caption", "")
                slide_id = row.get("slide_id", "")
                dataset_name_field = row.get("dataset", "")
            
            data.append({
                "h5_path": h5_path,
                "question": question,
                "answer": answer,
                "caption": caption,
                "slide_id": slide_id,
                "dataset": dataset_name_field,
            })
            
            # Log first few samples for debugging
            if idx < 3:
                logger.info(f"Sample {idx}: h5_path={h5_path}, caption_length={len(caption)}")
        
        # Create dataset
        dataset = Dataset.from_list(data)
        logger.info(f"Created dataset with {len(dataset)} samples")
        
        # ğŸ”§ ä½¿ç”¨æ–°çš„åŠ¨æ€åŠ è½½è½¬æ¢å™¨
        converter = DynamicUnpuzzleDatasetConverter(
            dataset_attr=dataset_attr,
            data_args=data_args
        )
        return converter.convert(dataset, tokenizer)
        
    except Exception as e:
        logger.error(f"Error loading Unpuzzle dataset: {str(e)}")
        raise

def get_unpuzzle_vqa_dataset(
    dataset_name: str,
    dataset_dir: str,
    tokenizer: "PreTrainedTokenizer",
    **kwargs
) -> Dataset:
    """Load and process the Unpuzzle WSI dataset for VQA (Stage 3) training."""
    try:
        # Get dataset attributes
        dataset_attr = kwargs.get("dataset_attr")
        data_args = kwargs.get("data_args")
        
        # Stage3 VQA mode
        if dataset_name in ["pathtext_wsivqa_stage3_train", "pathtext_wsivqa_stage3_val", "pathtext_wsivqa_stage3_test"]:
            # æ–°çš„ç»Ÿä¸€VQAæ•°æ®é›†æ¨¡å¼
            split = dataset_name.split('_')[-1]  # train, val, or test
            csv_filename = f"PathText-WSIVQA-BRCA-unpuzzle-{split}.csv"
            csv_path = f"/data/private_hdd/PathText-unpuzzle/{csv_filename}"
            split_name = f"Unified WSI-VQA {split} set"
            logger.info(f"{split_name}: {csv_path}")
        else:
            # Construct CSV path for other datasets
            csv_path = os.path.join(dataset_dir, f"{dataset_name}.csv")
        
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        # Read CSV file - handle both comma and tab separators
        try:
            df = pd.read_csv(csv_path, sep=",", keep_default_na=False)  # Try comma first
        except:
            df = pd.read_csv(csv_path, sep="\t", keep_default_na=False)  # Fallback to tab
        
        # å…³é”®ä¿®æ­£ï¼šå°†æ‰€æœ‰NaNå€¼æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œä»¥é˜²æ­¢pyarrowåœ¨å¤„ç†æ··åˆç±»å‹ï¼ˆstrå’Œfloatï¼‰æ—¶å‡ºé”™ã€‚
        df.fillna("", inplace=True)

        logger.info(f"Loaded {len(df)} samples from {csv_path}")

        # ğŸ†• æ·»åŠ æ•°æ®æºè¿‡æ»¤é€»è¾‘
        if data_args and data_args.filter_by_data_source:
            logger.info(f"Filtering dataset by data_source: '{data_args.filter_by_data_source}'")
            original_count = len(df)
            df = df[df["data_source"] == data_args.filter_by_data_source]
            logger.info(f"Filtered {original_count} -> {len(df)} samples.")
            if len(df) == 0:
                logger.warning("Warning: The dataset is empty after filtering.")

        logger.info(f"CSV columns: {df.columns.tolist()}")
        
        # ğŸ” å¦‚æœä½¿ç”¨é¢„å…ˆåˆ’åˆ†çš„æ•°æ®ï¼Œè®°å½•åˆ’åˆ†ä¿¡æ¯
        if 'split' in df.columns:
            split_counts = df['split'].value_counts()
            logger.info(f"ğŸ“Š æ•°æ®åˆ’åˆ†ç»Ÿè®¡: {split_counts.to_dict()}")
            # åªä½¿ç”¨è®­ç»ƒæ•°æ®
            if 'train' in split_counts:
                df = df[df['split'] == 'train'].copy()
                logger.info(f"ğŸ¯ ä½¿ç”¨è®­ç»ƒé›†æ•°æ®: {len(df)} ä¸ªæ ·æœ¬")
        
        # Convert DataFrame to list of dicts
        data = []
        discarded_samples_count = 0  # Initialize a counter for bad samples
        csv_dir = os.path.dirname(os.path.abspath(csv_path))
        
        for idx, row in df.iterrows():
            # Handle different CSV formats
            if "filepath" in df.columns and "title" in df.columns:
                # PathText-unpuzzle format: filepath, subset, title
                h5_path = os.path.join("/data/private_hdd/PathText-unpuzzle", row["filepath"])
                caption = row["title"]
                question = ""
                answer = ""
                slide_id = os.path.basename(row["filepath"]).replace(".h5", "")
                dataset_name_field = row.get("subset", "PathText")
            else:
                # Standard format: h5_path, question, answer, caption, etc.
                h5_path = row.get("h5_path", "")
                if not os.path.isabs(h5_path):
                    h5_path = os.path.join(csv_dir, h5_path)
                question = row.get("question", "").strip()
                answer = row.get("answer", "").strip()
                slide_id = row.get("slide_id", "") # Get slide_id early for logging
                
                # ğŸ†• Injected Prompt Engineering Logic for PathText samples (Case-insensitive)
                if row.get("data_source", "").lower() == "pathtext":
                    question = f"{question}\n\n**TASK:**\nStyle: Comprehensive Pathology Report"
                
                # ğŸ”§ Enhance logic: if choices exist, format and append them to the question
                choices_str = str(row.get("choices", "")) # Ensure it's a string
                is_choice_question = False
                if choices_str and choices_str != '[]':
                    is_choice_question = True
                    try:
                        # Safely parse the string representation of the list
                        choices_list = ast.literal_eval(choices_str)
                        if isinstance(choices_list, list) and choices_list:
                            # Format options as A) B) C) ...
                            formatted_choices = [f"{chr(65+i)}) {choice}" for i, choice in enumerate(choices_list)]
                            choices_text = "\n".join(formatted_choices)
                            question = f"{question}\n\nPlease select from the following options:\n{choices_text}"
                            
                            # Match and reformat the answer, or identify as a bad sample
                            answer_lower = answer.strip().lower()
                            choices_lower = [c.strip().lower() for c in choices_list]
                            
                            found_idx = -1

                            # Priority 1: Direct, exact match (case-insensitive)
                            try:
                                found_idx = choices_lower.index(answer_lower)
                            except ValueError:
                                # Priority 2: Unique substring match (for wrapper text like "the correct option(...)")
                                possible_matches_substring = [i for i, choice in enumerate(choices_lower) if choice in answer_lower]
                                if len(possible_matches_substring) == 1:
                                    found_idx = possible_matches_substring[0]
                                else:
                                    # Priority 3: Unique prefix match (for noise like "answer." or "answer4.")
                                    possible_matches_prefix = [i for i, choice in enumerate(choices_lower) if answer_lower.startswith(choice)]
                                    if len(possible_matches_prefix) == 1:
                                        found_idx = possible_matches_prefix[0]

                            if found_idx != -1:
                                correct_choice_text = choices_list[found_idx]
                                answer = f"{chr(65 + found_idx)}) {correct_choice_text}"
                            else:
                                # â€¼ï¸ This is a bad sample, log and discard it
                                discarded_samples_count += 1
                                logger.warning(
                                    f"DISCARDING sample. Answer not found in choices. "
                                    f"slide_id: {slide_id}, answer: '{answer}', choices: {choices_list}"
                                )
                                continue # Skip to the next row in the dataframe

                    except (ValueError, SyntaxError):
                        logger.warning(f"Could not parse choices string: {choices_str}, appending as is.")
                
                caption = row.get("caption", "")
                dataset_name_field = row.get("dataset", "")
            
            data.append({
                "h5_path": h5_path,
                "question": question,
                "answer": answer,
                "caption": caption,
                "slide_id": slide_id,
                "dataset": dataset_name_field,
            })
            
            # Log first few samples for debugging
            if idx < 3:
                logger.info(f"VQA Sample {idx}: h5_path={h5_path}, caption_length={len(caption)}, question_length={len(question)}, answer_length={len(answer)}")
        
        if discarded_samples_count > 0:
            logger.info(f"âœ… Total discarded samples due to answer/choice mismatch: {discarded_samples_count}")

        # Create dataset
        dataset = Dataset.from_list(data)
        logger.info(f"Created VQA dataset with {len(dataset)} samples")
        
        # ğŸ”§ ä½¿ç”¨æ–°çš„VQAè½¬æ¢å™¨
        converter = DynamicUnpuzzleVQADatasetConverter(
            dataset_attr=dataset_attr,
            data_args=data_args
        )
        return converter.convert(dataset, tokenizer)
        
    except Exception as e:
        logger.error(f"Error loading Unpuzzle VQA dataset: {str(e)}")
        raise

class DynamicUnpuzzleDatasetConverter(DatasetConverter):
    """åŠ¨æ€åŠ è½½WSIæ•°æ®é›†è½¬æ¢å™¨ - é¿å…é¢„åŠ è½½æ‰€æœ‰H5æ–‡ä»¶åˆ°å†…å­˜"""
    
    def __init__(self, dataset_attr: "DatasetAttr", data_args: "DataArguments"):
        super().__init__(dataset_attr, data_args)
        self.tokenizer = None
        self.transforms = None
        
        # WSI processing parameters
        self.h5_key_feat = "features"
        self.h5_key_coord = "coords_yx"
        self.max_patches = getattr(data_args, "patches_per_wsi", 1024)
        
    def convert(self, dataset: Dataset, tokenizer: "PreTrainedTokenizer", transforms=None) -> Dataset:
        """Convert the dataset to LLaMA-Factory format - å»¶è¿ŸåŠ è½½H5æ–‡ä»¶"""
        self.tokenizer = tokenizer
        self.transforms = transforms
        
        def convert_example(example: Dict) -> Dict:
            # ğŸ”§ å»¶è¿ŸåŠ è½½ï¼šåªå­˜å‚¨H5è·¯å¾„å’Œå…ƒæ•°æ®ï¼Œä¸åŠ è½½ç‰¹å¾æ•°æ®
            h5_path = example.get("h5_path")
            
            # åªéªŒè¯æ–‡ä»¶å­˜åœ¨æ€§ï¼Œä¸åŠ è½½ç‰¹å¾æ•°æ®
            if h5_path and not os.path.exists(h5_path):
                logger.warning(f"H5 file not found: {h5_path}")
                h5_path = None
            
            # ğŸ¯ Stage2ç®€åŒ–ï¼šåªéœ€è¦ WSI + Caption å¯¹åº”å…³ç³» 
            # å‚è€ƒLLaVAå®˜æ–¹Stage2ä½¿ç”¨CC-3Mçš„ç®€å•image-captionæ ¼å¼
            question = example.get("question", "").strip()
            answer = example.get("answer", "").strip()
            caption = example.get("caption", "").strip()
            
            if question and answer:
                # Stage3 VQA format: ä¿æŒå¤æ‚æ ¼å¼ç”¨äºåç»­VQAè®­ç»ƒ
                user_content = f"<image>\n{question}"
                assistant_content = answer
            elif caption:
                # ğŸ”§ Stage2ä¿®æ­£æ ¼å¼: æ˜ç¡®çš„æŒ‡ä»¤ï¼Œè€Œä¸æ˜¯ç®€å•çš„ç»­å†™
                # å‚è€ƒLLaVAå®˜æ–¹åšæ³•ï¼šå›¾åƒæ ‡è®° + ä¸€ä¸ªå›ºå®šçš„ã€é€šç”¨çš„é—®é¢˜
                question_text = "Please provide a comprehensive pathology report for this whole-slide image."
                user_content = f"<image>\n{question_text}"
                assistant_content = caption  # ä½¿ç”¨captionä½œä¸ºresponse
            else:
                # Fallback: ä½¿ç”¨åŸºæœ¬æè¿°
                slide_id = example.get("slide_id", "unknown")
                user_content = f"<image>"
                assistant_content = f"This is a histopathological slide from slide {slide_id}."
            
            # ç®€åŒ–çš„å¯¹è¯æ ¼å¼ - å»æ‰ä¸å¿…è¦çš„å¤æ‚æ€§
            prompt = [{"role": "user", "content": user_content}]
            response = [{"role": "assistant", "content": assistant_content}]
            
            return {
                "_prompt": prompt,
                "_response": response,
                "_system": "",  # ğŸ¯ Stage2ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„system message
                "_tools": "",
                "_images": [h5_path] if h5_path else [],  # ğŸ”§ å­˜å‚¨H5è·¯å¾„è€Œä¸æ˜¯ç‰¹å¾æ•°æ®
                "_videos": None,
                "_audios": None,
                # ä¿ç•™å…³é”®å…ƒæ•°æ®ç”¨äºè°ƒè¯•
                "_slide_id": example.get("slide_id", ""),
                "_dataset": example.get("dataset", ""),
                "_h5_path": h5_path,
                "_max_patches": self.max_patches,  # ä¼ é€’patchæ•°é‡å‚æ•°
                # ğŸ”§ ç®€åŒ–ï¼šç§»é™¤åŸå§‹é—®ç­”å­—æ®µï¼Œä¸“æ³¨äºcaption
                "_original_caption": caption,
            }
        
        return dataset.map(
            convert_example,
            remove_columns=dataset.column_names,
            desc="Converting Unpuzzle dataset to LLaMA-Factory format (Dynamic Loading)"
        )

class UnpuzzleDatasetConverter(DatasetConverter):
    """Convert Unpuzzle WSI dataset to LLaMA-Factory format."""
    
    def __init__(self, dataset_attr: "DatasetAttr", data_args: "DataArguments"):
        super().__init__(dataset_attr, data_args)
        self.tokenizer = None
        self.transforms = None
        
        # WSI processing parameters
        self.h5_key_feat = "features"
        self.h5_key_coord = "coords_yx"
        self.max_patches = getattr(data_args, "patches_per_wsi", 1024)
        
    def convert(self, dataset: Dataset, tokenizer: "PreTrainedTokenizer", transforms=None) -> Dataset:
        """Convert the dataset to LLaMA-Factory format."""
        self.tokenizer = tokenizer
        self.transforms = transforms
        
        def convert_example(example: Dict) -> Dict:
            # ğŸ”§ å»¶è¿ŸåŠ è½½ï¼šåªå­˜å‚¨H5è·¯å¾„ï¼Œä¸åœ¨æ­¤é˜¶æ®µåŠ è½½ç‰¹å¾
            h5_path = example.get("h5_path")
            
            # åªéªŒè¯æ–‡ä»¶å­˜åœ¨æ€§ï¼Œä¸åŠ è½½ç‰¹å¾æ•°æ®
            if h5_path and not os.path.exists(h5_path):
                logger.warning(f"H5 file not found: {h5_path}")
                h5_path = None
            
            # ğŸ”§ Stage2ä¿®æ­£æ ¼å¼: ä¸DynamicUnpuzzleDatasetConverterä¿æŒä¸€è‡´
            question = example.get("question", "").strip()
            answer = example.get("answer", "").strip()
            caption = example.get("caption", "").strip()
            
            if question and answer:
                # Stage3 VQA format: ä¿æŒå®Œæ•´æ ¼å¼
                user_content = f"<image>\n{question}"
                assistant_content = answer
            elif caption:
                # ğŸ”§ Stage2ä¿®æ­£æ ¼å¼: å›¾åƒæ ‡è®° + captionå†…å®¹
                user_content = f"<image>"
                assistant_content = caption  # ä½¿ç”¨captionä½œä¸ºresponse
            else:
                # Fallback: ä¿®æ­£æ ¼å¼
                slide_id = example.get("slide_id", "unknown")
                user_content = f"<image>"
                assistant_content = f"This is a histopathological slide from slide {slide_id}."
            
            # ç®€åŒ–çš„å¯¹è¯æ ¼å¼
            prompt = [{"role": "user", "content": user_content}]
            response = [{"role": "assistant", "content": assistant_content}]
            
            return {
                "_prompt": prompt,
                "_response": response,
                "_system": "",  # ğŸ¯ Stage2ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„system message
                "_tools": "",
                "_images": [h5_path] if h5_path else [],  # ğŸ”§ å­˜å‚¨H5è·¯å¾„è€Œä¸æ˜¯ç‰¹å¾æ•°æ®
                "_videos": None,
                "_audios": None,
                # ä¿ç•™å…³é”®å…ƒæ•°æ®
                "_slide_id": example.get("slide_id", ""),
                "_dataset": example.get("dataset", ""),
                "_h5_path": h5_path,
                # ğŸ”§ ç®€åŒ–ï¼šä¸“æ³¨äºcaption
                "_original_caption": caption,
            }
        
        return dataset.map(
            convert_example,
            remove_columns=dataset.column_names,
            desc="Converting Unpuzzle dataset to LLaMA-Factory format"
        )

    def __call__(self, example: Dict) -> Dict:
        """Convert a single example to LLaMA-Factory format."""
        # ğŸ”§ Stage2ä¿®æ­£æ ¼å¼ï¼šä¸ä¸»è¦è½¬æ¢é€»è¾‘ä¿æŒä¸€è‡´
        question = example.get("question", "").strip()
        answer = example.get("answer", "").strip()
        caption = example.get("caption", "").strip()
        
        if question and answer:
            user_content = f"<image>\n{question}"
            assistant_content = answer
        elif caption:
            user_content = f"<image>"  # ä¿®æ­£æ ¼å¼
            assistant_content = caption  # ä½¿ç”¨captionä½œä¸ºresponse
        else:
            user_content = f"<image>"
            assistant_content = "This is a histopathological slide."
        
        return {
            "_prompt": [{"role": "user", "content": user_content}],
            "_response": [{"role": "assistant", "content": assistant_content}],
            "_system": "",  # ğŸ¯ Stage2ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„system message
            "_tools": "",
            "_images": None,  # Will be processed during actual conversion
            "_videos": None,
            "_audios": None,
        } 

class DynamicUnpuzzleVQADatasetConverter(DatasetConverter):
    """åŠ¨æ€åŠ è½½WSIæ•°æ®é›†è½¬æ¢å™¨ - Stage3 VQAæ¨¡å¼ï¼Œå°†captionè½¬æ¢ä¸ºé—®ç­”å¯¹"""
    
    def __init__(self, dataset_attr: "DatasetAttr", data_args: "DataArguments"):
        super().__init__(dataset_attr, data_args)
        self.tokenizer = None
        self.transforms = None
        
        # WSI processing parameters
        self.h5_key_feat = "features"
        self.h5_key_coord = "coords_yx"
        self.max_patches = getattr(data_args, "patches_per_wsi", 1024)
        
    def convert(self, dataset: Dataset, tokenizer: "PreTrainedTokenizer", transforms=None) -> Dataset:
        """Convert the dataset to LLaMA-Factory format with VQA question generation"""
        self.tokenizer = tokenizer
        self.transforms = transforms
        
        def convert_example(example: Dict) -> Dict:
            # ğŸ”§ å»¶è¿ŸåŠ è½½ï¼šåªå­˜å‚¨H5è·¯å¾„å’Œå…ƒæ•°æ®ï¼Œä¸åŠ è½½ç‰¹å¾æ•°æ®
            h5_path = example.get("h5_path")
            
            # åªéªŒè¯æ–‡ä»¶å­˜åœ¨æ€§ï¼Œä¸åŠ è½½ç‰¹å¾æ•°æ®
            if h5_path and not os.path.exists(h5_path):
                logger.warning(f"H5 file not found: {h5_path}")
                h5_path = None
            
            # ğŸ¯ Stage3 VQAæ¨¡å¼ï¼šä¼˜å…ˆç”Ÿæˆé—®ç­”å¯¹
            question = example.get("question", "").strip()
            answer = example.get("answer", "").strip()
            caption = example.get("caption", "").strip()
            slide_id = example.get("slide_id", "unknown")
            
            generated_question = None  # åˆå§‹åŒ–å˜é‡
            
            if question and answer:
                # å·²æœ‰é—®ç­”å¯¹ï¼Œç›´æ¥ä½¿ç”¨
                user_content = f"<image>\n{question}"
                assistant_content = answer
                logger.debug(f"ä½¿ç”¨ç°æœ‰é—®ç­”å¯¹: slide_id={slide_id}")
            elif caption:
                # ğŸ”§ Stage3æ ¸å¿ƒï¼šè‡ªåŠ¨ç”Ÿæˆé—®ç­”å¯¹
                # ä½¿ç”¨slide_idç¡®ä¿åŒä¸€ä¸ªslideæ€»æ˜¯å¾—åˆ°ç›¸åŒçš„é—®å¥
                generated_question = make_vqa_question(slide_id)
                user_content = f"<image>\n{generated_question}"
                assistant_content = caption  # captionä½œä¸ºç­”æ¡ˆ
                logger.debug(f"ç”ŸæˆVQAé—®ç­”å¯¹: slide_id={slide_id}, question={generated_question[:50]}...")
            else:
                # Fallback: ä½¿ç”¨åŸºæœ¬é—®ç­”
                generated_question = "What does this whole-slide image show?"
                user_content = f"<image>\n{generated_question}"
                assistant_content = f"This is a histopathological slide from slide {slide_id}."
                logger.debug(f"ä½¿ç”¨fallbacké—®ç­”: slide_id={slide_id}")
            
            # æ ‡å‡†çš„å¯¹è¯æ ¼å¼
            prompt = [{"role": "user", "content": user_content}]
            response = [{"role": "assistant", "content": assistant_content}]
            
            return {
                "_prompt": prompt,
                "_response": response,
                "_system": "",  # ğŸ¯ Stage3ç®€åŒ–ï¼šç§»é™¤å¤æ‚çš„system message
                "_tools": "",
                "_images": [h5_path] if h5_path else [],  # ğŸ”§ å­˜å‚¨H5è·¯å¾„è€Œä¸æ˜¯ç‰¹å¾æ•°æ®
                "_videos": None,
                "_audios": None,
                # ä¿ç•™å…³é”®å…ƒæ•°æ®ç”¨äºè°ƒè¯•
                "_slide_id": slide_id,
                "_dataset": example.get("dataset", ""),
                "_h5_path": h5_path,
                "_max_patches": self.max_patches,  # ä¼ é€’patchæ•°é‡å‚æ•°
                # ğŸ”§ VQAæ¨¡å¼ï¼šä¿ç•™åŸå§‹æ•°æ®
                "_original_caption": caption,
                "_original_question": question,
                "_original_answer": answer,
                "_generated_question": generated_question if not question else None,
            }
        
        return dataset.map(
            convert_example,
            remove_columns=dataset.column_names,
            desc="Converting Unpuzzle dataset to LLaMA-Factory VQA format (Dynamic Loading)"
        ) 